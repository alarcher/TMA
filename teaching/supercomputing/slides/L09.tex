\input{preamble}

%\newcommand{\xC}{\mathbb{C}}
\newcommand{\xR}{\mathbb{R}}
\newcommand{\xRd}{{\xR^d}}
\newcommand{\xRN}{{\xR^N}}
\newcommand{\xMNR}{{M_N(\xR)}}
\newcommand{\bb}{{\boldsymbol b}}
\newcommand{\ee}{{\boldsymbol e}}
\newcommand{\ev}{{\boldsymbol \epsilon}}
\newcommand{\rr}{{\boldsymbol r}}
\newcommand{\xx}{{\boldsymbol x}}
\newcommand{\hx}{\hat{\boldsymbol x}}
\newcommand{\yy}{{\boldsymbol y}}
\newcommand{\vv}{{\boldsymbol v}}
\newcommand{\ww}{{\boldsymbol w}}
\newcommand{\zz}{{\boldsymbol z}}
\renewcommand{\mA}{{\mathrm A}}
\newcommand{\mB}{{\mathrm B}}
\newcommand{\mC}{{\mathrm C}}
\newcommand{\mD}{{\mathrm D}}
\newcommand{\mG}{{\mathrm G}}
\newcommand{\mH}{{\mathrm H}}
\renewcommand{\mL}{{\mathrm L}}
\newcommand{\mLs}{{\mathrm L_0}}
\newcommand{\mM}{{\mathrm M}}
\newcommand{\mRs}{{\mathrm R_0}}
\newcommand{\mR}{{\mathrm R}}
\newcommand{\mP}{{\mathrm P}}
\newcommand{\mQ}{{\mathrm Q}}
\newcommand{\mU}{{\mathrm U}}
\newcommand{\mId}{{\mathbf{Id}}}
\newcommand{\mII}{{\mathbf{\mathbb{I}}}}
\newcommand{\Seq}[1]{\bigl(#1\bigr)}
\newcommand{\Cond}[1]{\mathcal{C}(#1)}
\newcommand{\Order}[1]{\mathcal{O}\left(#1\right)}
\newcommand{\norm}[1]{{\lVert #1 \rVert}}
\newcommand{\norminf}[1]{\norm{#1}_{\infty}}

\newcommand{\InnerK}[2]{{{\mathbf\langle}\;#1\:,\: #2 \;{\rangle}}}
\newcommand{\Inner}[2]{{{\scriptstyle\mathbf{(}}\;#1\:,\: #2 \;{\scriptstyle\mathbf{)}}}}


\title{Discussion on Iterative Solvers}
\institute{NTNU, IMF}
\date{April 6. 2018}
%\author{Based on 2016v slides by Eivind Fonn}
%\author{Aur√©lien Larcher}

\maketitle

\begin{frame}
  \frametitle{Problem}
  \begin{itemize}
  \item Solve
    \[
      \mA \bm x = \bb, \qquad
      \bb, \bm x \in \mathbb{R}^N, \qquad
      \mA \in \xMNR
    \]
    where $\mA$ can be the system resulting from discretizing a Poisson problem
    using finite differences.
  \bigskip
  \item We use standard notation for matrices and vectors, i.e.
    \[
      \begin{pmatrix}
        a_{1,1} & a_{1,2} & \cdots & a_{1,N} \\
        a_{2,1} & a_{2,2} & \cdots & a_{2,N} \\
        \vdots  & \vdots  & \ddots & \vdots  \\
        a_{N,1} & a_{N,2} & \cdots & a_{N,N}
      \end{pmatrix}
      \begin{pmatrix} x_1 \\ x_2 \\ \vdots \\ x_N \end{pmatrix} =
      \begin{pmatrix} b_1 \\ b_2 \\ \vdots \\ b_N \end{pmatrix}
    \]
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Direct methods}
  \begin{itemize}
  \item Computing the inverse of the matrix is unrealistic,
  \item Matrix inversion has the same time complexity as matrix multiplication
    (typically $\mathcal{O}(n^3)$).
  \item Direct methods can theoretically compute exact solutions $\xx\xRN$ to linear systems in the form of:
\[
\mA \xx = \bb
\]
  \item Several methods were introduced based on factorizations of the type $\mA = \mP\;\mQ$
  \item $\mP$ and $\mQ$ have a structure simplifying the resolution of the system: diagonal, banded, triangular.
  \item The structure and properties of the matrix $\bm A$ determine which
    algorithms we can use.
  \end{itemize}

Ex: $\mL\mU$, Cholevski involve triangular matrices, $\mQ\mR$  constructs an orthogonal basis.
\end{frame}

\begin{frame}
  \frametitle{Iterative methods}

All methods prove to be quite expensive, hard to parallelize due to the sequential nature of the algorithm and prone to error propagation.

\medskip
Iterative methods have been developed for:
\begin{itemize}
\item solving very large linear systems with direct methods is in practice not possible due to the complexity in term of computational operations and data,
\item taking advantage of sparse system for which the structure of the matrix can result in dramatic speed-up (this is the case for numerical schemes for PDEs),
\item using the fact that some systems like PDEs discretizations are already formulated in an iterative fashion.
\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{General idea}

Introduce a splitting of the form:
\[
\mA = \mG - \mH
\]
such the solution $\xx$ satisfies:
\[
\mG \xx = \bb + \mH \xx
\]

Similarly to fixed-point methods we can define a sequence of approximate solutions $\Seq{\xx^k}$ satisfying relations of the form:
\[
\mG \hx^{k+1} = \bb + \mH \hx^{k}
\]
with $\mG$ invertible.

\end{frame}

\begin{frame}
  \frametitle{Link to linear mappings}

The matrix viewed as a linear mapping in $\xRN$:
\begin{itemize}
\item the counterpart embodied by the Brouwer Theorem in finite dimension,
\item Continuous mapping $f : \Omega \rightarrow \Omega$ with $\Omega$ compact of $\xRN$
\begin{enumerate}
\item Admits a fixed-point $\xx^\star$ satisfying $f(\xx^\star) = \xx^\star$,
\item and is contracting.
\end{enumerate}
\end{itemize}

\bigskip
Following a sequence of approximate solutions $\Seq{\xx^k}$ in $\xRN$

\end{frame}

\begin{frame}
  \frametitle{Computational aspects}

Methods introduced depend on the iteration defined by the splitting:
\begin{enumerate}
\item How can the convergence be ensured?
\item How fast is the convergence?
\item How expensive is each iteration?
\item How does the algorithm behave with respect to numerical error?
\end{enumerate}

\medskip
Estimate on error vectors in terms of iteration error $\hat{\ev}^k = \hx^{k+1}-\hx^{k}$ or global error: $\ev^k = \hx^{k}-\xx$.
With $\mA = \mG - \mH$
\[
\hat{\ev}^{k} = \mG^{-1}\mH\;\hat{\ev}^{k-1}
\]

\medskip
Existence of a contraction factor $K < 1$ such that $\norminf{\hat{\ev}^{k}} \leq K\;\norminf{\hat{\ev}^{k-1}}$

\medskip
Spectral radius $\rho(\mM)$ as $\rho(\mM) < 1$ since in that case $\lim_{k\rightarrow\infty} M^k \hat{\ev}^{0} = 0_{\xRN}$.
The smaller the spectral radius, the faster the convergence.

\end{frame}

\begin{frame}
  \frametitle{Jacobi, methods of simultaneous displacements}

\begin{equation}
\hx^{k+1}_i = \frac{1}{a_{ii}}\Bigl( b_i - \sum_{i\neq j} a_{ij} \hx^{k}_j \Bigr)
\end{equation}

\medskip
\textbf{Convergence:} the global error $\ev^k$ is controlled by
\begin{equation*}
\norm{\ev^{k+1}} \leq \sum_{i\neq j} \Bigl\lvert\frac{a_{ij}}{a_{ii}}\Bigr\rvert\;\norm{\ev^{k}} \leq K^k \; \norm{\ev^{1}}
\end{equation*}
It is then enough if the matrix is strictly diagonally dominant.
Expressing the iteration error gives $\mM = \mG^{-1}\mH$ such that $\rho(\mM)< 1$.

\begin{enumerate}
\item Parallelization component by component is possible since there is only dependency on $\hx^{k}$.
\item Memory requirement for storing both $\hx^{k+1}$ and $\hx^{k}$ at each iteration.
\end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Gauss--Seidel, methods of sucessive displacements}

In Jacobi iterations, notice that sequential ordered computation of terms
\begin{equation}
\hx^{k+1}_i = \frac{1}{a_{ii}}\Bigl( b_i - \sum_{i\neq j} a_{ij} \hx^{k}_j \Bigr)
\end{equation}
involves components $\hx^{k}_j$ which are also computed for $\hx^{k+1}$ if $j < i$.
\begin{equation}
\hx^{k+1}_i = \frac{1}{a_{ii}}\Bigl( b_i - \sum_{i > j} a_{ij} \hx^{k+1}_j - \sum_{i < j} a_{ij} \hx^{k}_j \Bigr)
\end{equation}

\medskip
\textbf{Algorithm:} the splitting is
\[
\mA = \mL - \mRs
\]
with $\mL = \mD + \mLs$ lower-triangular matrix and $\mRs$ strict upper-triangular matrix, thus
\[
\hx^{k+1} = \mD^{-1}(\bb - \mLs \hx^{k+1} + \mRs \hx^{k})
\]

\end{frame}

\begin{frame}
  \frametitle{Gauss--Seidel, methods of sucessive displacements}

Recast under the usual form:
\[
\hx^{k+1} = \mL^{-1}(\bb + \mRs \hx^{k})
\]
and the iteration matrix is $\bar \mM = \mL^{-1}\mRs$.

\medskip
\textbf{Convergence:} the global error $\ev^k$ is controlled by
\begin{equation*}
\norm{\ev^{k+1}} \leq \frac{\displaystyle\sum_{i < j} \Bigl\lvert\frac{a_{ij}}{a_{ii}}\Bigr\rvert}{1 - \displaystyle\sum_{i > j} \Bigl\lvert\frac{a_{ij}}{a_{ii}}\Bigr\rvert}\;\norm{\ev^{k}} \leq \bar{K}^k \; \norm{\ev^{1}}
\end{equation*}
If the Jacobi contraction factor $K < 1$ then $\bar{K} < 1$.
Expressing the iteration error gives directly that $\bar\mM = \mL^{-1}\mRs$ such that $\rho(\bar\mM)< 1$.

\end{frame}

\begin{frame}
  \frametitle{Gauss--Seidel, methods of sucessive displacements}

\medskip
\textbf{Implementation:}
\begin{enumerate}
\item Parallelization component by component is not possible easily since there is serialization for each row $i$ due to the dependency on $\hx_j^{k+1}$, $j < i$.
\item Memory requirement is only for storing one vector of $\xRN$ at each iteration.
\end{enumerate}

\medskip
Parallelization possible if the matrix is sparse: components do not all possess connectivities with each other:
\begin{enumerate}
\item Component Dependency-Graph: generate a graph to reorder entries such that dependencies are avoided.
\item Red--Black coloring: special case for two-dimensional problems.
\end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Relaxation methods}

Introduce the relaxation parameter $\gamma \in (0,1)$: adding a linear combination of the approximate solution at the previous iteration to minimize the spectral radius for convergence.

\medskip
The relaxation parameter $\gamma$ cannot be known \textit{a priori} and is usually determined by heuristics.

\end{frame}

\begin{frame}
  \frametitle{Relaxation methods}

\begin{enumerate}
\item Jacobi Over-Relaxation (JOR):
\begin{equation}
\hx^{k+1}_i = (1 - \gamma)\;\hx^{k}_i + \gamma \frac{1}{a_{ii}}\Bigl( b_i - \sum_{i\neq j} a_{ij} \hx^{k}_j \Bigr)
\end{equation}
which reads in matricial form
\[
\hx^{k+1} = \mM_\gamma \hx^{k} + \gamma\mD^{-1}\;\bb
\]
with $\mM_\gamma = (1-\gamma)\mII + \gamma\mD^{-1} \mH$

\medskip
\item Successive Over-Relaxation (SOR):
\begin{equation}
\hx^{k+1}_i = (1-\gamma)\;\hx^{k}_i + \gamma \frac{1}{a_{ii}}\Bigl( b_i - \sum_{i > j} a_{ij} \hx^{k+1}_j - \sum_{i < j} a_{ij} \hx^{k}_j \Bigr)
\end{equation}
which reads in matricial form
\[
\hx^{k+1} = \mM_\gamma \hx^{k} + \gamma\mC\;\bb
\]
with $\mM_\gamma = (1 + \gamma\mD^{-1}\mLs^{-1} )^{-1} \bigl[(1-\gamma)\mII + \gamma\mD^{-1} \mRs \bigr]$ and $\mC = (1 + \gamma\mD^{-1}\mLs^{-1} )^{-1} \mD^{-1}$
\end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Krylov-subspace methods}

Idea: decomposition on a sequence of orthogonal subspaces.

\bigskip
If $\mA$ is symmetric definite positive it induces the corresponding scalar product:
\[
\InnerK{\xx}{\yy} = \Inner{\mA \xx}{\yy} = \yy^T\mA\xx
\]
with $\Inner{\mA \cdot}{\cdot}$ canonical scalar product in $\xRN$.
The vectors $(\ee_1, \dots, \ee_N)$ are said $\mA$-conjugate if $\ee_j^T\mA\ee_i = 0$ for $i\neq j$: they are orthogonal for the scalar-product induced by $\mA$.

\bigskip
To bring the Conjugate Gradient method, first let us introduce the idea of descent method.

\end{frame}

\begin{frame}
  \frametitle{Descent methods}

Minimisation of the residual:
\begin{equation*}
\xx^\star = \mathrm{argmin}_\xx \:{\rm J}(\xx) = \frac{1}{2}\InnerK{\xx}{\xx} - \InnerK{\bb}{\xx}
\end{equation*}

Construct a sequence of solutions to approximate minimization problems, given $\hx^k$:
\[
{\rm J}(\hx^{k+1}) \leq {\rm J}(\hx^k)
\]
where $\hx^{k+1} = \hx^{k} + \alpha_{k+1} \ee^{k+1}$, with $\alpha_{k+1}$ a descent factor and $\ee_{k+1}$ a direction.

\end{frame}

\begin{frame}
  \frametitle{Steepest Gradient}

For the Steepest Gradient:
\begin{enumerate}
\item take the direction given by $-\nabla{\rm J}(\hx^k) = \bb - \mA \hx^k$ which is the residual $\rr_k = \bb - \mA\hx^k$, thus $\hx^{k+1} = \hx^{k} + \alpha_{k+1} \rr_k$.
\item choose the descent factor $\alpha^{k+1}$ minimizing the functional ${\rm J}(\hx^{k} + \alpha_{k+1} \rr_k)$:
\[
\alpha_{k+1} = \frac{\rr_k^T\bb}{\rr_k^T\mA\rr_k}
\]
\end{enumerate}

\medskip
\begin{enumerate}
\item Speed of convergence is bounded by $\Order{1- \Cond{\mA}^{-1}}$ with $\Cond{\mA}$ the conditioning of $\mA$.
\item Gradient direction may not be optimal: Conjugate Gradient methods improve the choice of $\Seq{\ee_k}$.
\end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Conjugate Gradient}

The Conjugate Gradient (CG) is a Krylov-subspace algorithm for symmetric positive definite matrices.

\medskip
Given $\hx^0$, $\Seq{\hx^k}$ is q sequence of solutions to approximate $k$-dimensional minimisation problems.

\medskip
For the Conjugate Gradient:
\begin{enumerate}
\item take the direction $\ee_{k+1}$ such that $\bigl(\ee_1, \dots, \ee_{k}, \ee_{k+1}\bigr)$ is $A$-conjugate, thus $\hx^{k+1} = \hx^{k} + \alpha_{k+1} \ee_{k+1}$.
\item In doing so we build an orthogonal basis for the scalar product given by A (Gram-Schmidt)
\item choose the descent factor $\alpha^{k+1}$ minimizing the functional ${\rm J}(\hx^{k} + \alpha_{k+1} \rr_k)$, which is defined by
\[
\alpha_j = \frac{\ee^T_j\bb}{\ee^T_j\mA\ee_j}
\]
and with $\ee^T_j\bb \neq 0$ (unless the exact solution is reached).
\end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Conjugate Gradient}

\medskip
The construction of $\bigl(\ee_1, \dots, \ee_{k+1}\bigr)$ is done by orthogonalization of residuals by Gram--Schmidt:
\[
\ee_{k+1} = \rr_k - \frac{\ee^T_k\mA\rr_{k-1}}{\ee^T_k\mA\ee_k}\ee_k
\]
so that $\rr_{k+1} = \bb - \mA\hx^{k+1} = \rr^k - \alpha_{k+1}\mA\ee_{k+1}$

\medskip
After $N$ steps, the $A$-conjugate basis of $\xRN$ is done and the exact solution is reached:
\[
 \xx = \sum_{j=1}^N \alpha_j \hx^j
\]

\end{frame}

\begin{frame}
  \frametitle{Conjugate Gradient}

For any $k$, the speed of convergence is bounded by
\[
\Order{ \frac{ 1-\sqrt{\Cond{\mA}} } { 1+\sqrt{\Cond{\mA}} } }^{2k}
\]
in the norm induced by $A$, with $\Cond{\mA}$ the conditioning of $\mA$.

\medskip
The Conjugate Gradient can therefore be seen as a direct methods but in practice:
\begin{itemize}
\item the iterative computation of the $A$-conjugate basis suffers from the same issue of numerical error propagation as the $\mQ\mR$ factorization leading to a loss of orthogonality,
\item the convergence is slow, which makes it unrealistic to compute the exact solution for large systems,
\end{itemize}
so it is used as an iterative method.

\end{frame}

\begin{frame}
  \frametitle{Conjugate Gradient}

First steps:
\begin{enumerate}
\item Given $\hx^0 = 0$, set $\rr_0 = \bb - \mA \hx^0$ and $\ee_1 = \rr_0$,
\item Take $\hx_1 = \alpha_1 \ee_1$, then $\alpha_1\ee_1^T \mA \ee_1 = \ee_1^T \bb$, thus
\[
\alpha_1 = \frac{\rr^T_0\bb}{\rr^T_0\mA\rr_0}
\]
\item Compute the residual:
\[
\rr_1 = \bb - \mA \hx^1
\]
\item Compute the direction:
\[
\ee_{2} = \rr_1 - \frac{\ee^T_1\mA\rr_{0}}{\ee^T_1\mA\ee_1}\ee_{1}
\]
\item Compute the factor:
\[
\alpha_2 = \frac{\ee^T_2\bb}{\ee^T_2\mA\ee_2}
\]
\item Update the solution:
\[
\hx^2 = \hx^1 + \alpha_2 \ee_2
\]
\item \dots
\end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Conjugate Gradient}

The algorithm iteration reads:
\begin{enumerate}
\item Compute the residual:
\[
\rr_k = \bb - \mA \hx^{k}
\]
\item Compute the direction:
\[
\ee_{k+1} = \rr_k - \frac{\ee^T_{k}\mA\rr_{k-1}}{\ee^T_k\mA\ee_k}\ee_k
\]
\item Compute the factor:
\[
\alpha_{k+1} = \frac{\ee^T_{k+1}\bb}{\ee^T_{k+1}\mA\ee_{k+1}}
\]
\item Update the solution:
\[
\hx^{k+1} = \hx^k + \alpha_{k+1} \ee_{k+1}
\]\end{enumerate}
which requires two matrix-vector multiplications per loop, $\mA\hx^{k}$ then $\mA\ee_{k+1}$
Using $\rr_{k+1} = \rr^k - \alpha_{k+1}\mA\ee_{k+1}$ saves one matrix-vector multiplication.

\end{frame}

\begin{frame}
  \frametitle{Conjugate Gradient}

While the residual norm $\varrho_k = \norm{\rr_k}_2$ is big:
\begin{enumerate}
\item Compute the projection:
\[
\beta_k = \frac{\varrho_k}{\varrho_{k-1}}
\]
\item Compute the direction:
\[
\ee_{k+1} = \rr_k + \beta_k\ee_k
\]
\item Compute the factor:
\[
\ww = \mA\ee_{k+1}; \alpha_{k+1} = \frac{\varrho_k}{\ee^T_{k+1}\ww}
\]
\item Update the solution:
\[
\hx^{k+1} = \hx^k + \alpha_{k+1} \ee_{k+1}
\]
\item Update the residual:
\[
\rr^{k+1} = \rr^k - \alpha_{k+1} \ww
\]
\end{enumerate}
Cost is one matrix-multiplication and five vector operations.

\end{frame}

\begin{frame}
  \frametitle{Preconditioners}

The convergence is still slow as soon as the condition number of the matrix is bad.

\medskip
Preconditioning the system consists in finding a non-singular symmetrix matrix $\mC$ such that $\tilde \mA = \mC^{-1}\mA\mC$ and the conjugate gradient is applied to
\[
\tilde\mA \tilde\xx = \tilde\bb
\]
with $\tilde\xx = \mC^{-1}\xx$ and $\tilde\bb = \mC^{-1}\bb$.

\medskip
With:
\begin{itemize}
\item $\mM = \mC^2$
\item $\ee_k = \mC^{-1}\tilde\ee_k$
\item $\hx_k = \mC^{-1}\tilde\hx_k$
\item $\zz_k = \mC^{-1}\tilde\rr_k$
\item $\rr_k = \mC\tilde\rr_k = \bb - \mA\hx_k$
\end{itemize}
and $\mM$ is a symmetric positive definite matrix called the preconditioner.

\end{frame}

\begin{frame}
  \frametitle{Preconditioned CG}

While the residual norm $\varrho_k = \norm{\rr_k}_2$ is big:
\begin{enumerate}

\item Solve:
\[
\mM\zz_{k} = \rr_k
\]
\item Compute the projection:
\[
\beta_k = \frac{\zz_{k}^T\rr_k}{\zz_{k-1}^T\rr_{k-1}}
\]
\item Compute the direction:
\[
\ee_{k+1} = \zz_k + \beta_k\ee_k
\]
\item Compute the factor:
\[
\alpha_{k+1} = \frac{\zz_{k}^T\rr_k}{\ee^T_{k+1}\mA\ee_{k+1}}
\]
\item Update the solution:
\[
\hx^{k+1} = \hx^k + \alpha_{k+1} \ee_{k+1}
\]
\item Update the residual:
\[
\rr^{k+1} = \rr^k - \alpha_{k+1} \ww
\]
\end{enumerate}

\end{frame}

\begin{frame}
  \frametitle{Preconditioned CG}

\medskip
The linear system $\mM\zz_{k} = \rr_k$ should be easy to solve and can lead to fast convergence, typically $\Order{\sqrt{N}}$.
Since
\[
\mM\zz_{k} = \bb - \mA\hx_k
\]
Then an iterative relation appears:
\[
\hx_{k+1} = \mM^{-1}\bigl(\bb - \mA\hx_k\bigr)
\]
therefore iterative methods like Jacobi, Gauss-Seidel and relaxation methods can be used.

\end{frame}


\begin{frame}
  \frametitle{Performance Analysis: Poisson in 2D}

Linear communication model:
\[
\tau_C{k} = \tau_S + \gamma\dot k
\]

For $N = n^2$ degrees of freedom:
\begin{itemize}
\item matrix-vector multiplication: $5$ multiplications and $4$ additions per row.
\item scale-vector multplication: $N$ multplications.
\item innner-product: $N$ multplications and $N-1$ additions.
\end{itemize}

Total operational cost in serial is:
\[
9N + 3N + 3N + 4N
\]
thus $T_1 = 19N {\cal N}_{CG} \tau_A$.

\medskip
For a one-directional paritioning:
\begin{itemize}
\item matrix-vector requires communication for $O(n)$ shared points per rank.
\item inner-products require a global reduction.
\end{itemize}

Total communication cost is:
\[
T_{comm} = 4 \tau_C(8n) + 2(\tau_A + \tau_C(8)) log_2(P)
\]

\end{frame}

\begin{frame}
  \frametitle{Performance Analysis: Poisson in 2D}


If $\tau_A << \tau_C(8)$ ans $\tau_C(8) \approx \tau_S$, total communication cost is:
\[
T_{comm} = 4 \tau_C(8n) + 2\tau_S log_2(P)
\]

\end{frame}

\begin{frame}
  \frametitle{Power method}

Find the dominant eigenvalues of a matrix $\mA \in \xMNR$ of $N$ eigenvectors $\Seq{\vv_i}$ with associated eigenvalues $\Seq{\lambda_i}$ ordered in decreasing module.
The eigenvalues are either real or conjugate complex pairs.

\medskip
Given a random vector $\xx^0$, construct a sequence of vectors $\Seq{\hx^k}$ such that
\[
\hx^{k+1} = \mA \hx^k
\]
then $\forall k \geq 0$
\[
\hx^k = \sum_{i=0}^{N-1} \lambda_i^k \xi_i \vv_i
\]
for some coefficients $\Seq{\vv_i}$.

\end{frame}

\begin{frame}
  \frametitle{Power method}

Find the dominant eigenvalues of a matrix $\mA \in \xMNR$ of $N$ eigenvectors $\Seq{\vv_i}$ with associated eigenvalues $\Seq{\lambda_i}$ ordered in decreasing module.
The eigenvalues are either real or conjugate complex pairs.

\medskip
Given a random vector $\xx^0$, construct a sequence of vectors $\Seq{\hx^k}$ such that
\[
\hx^{k+1} = \mA \hx^k
\]
then $\forall k \geq 0$
\[
\hx^k = \sum_{i=0}^{N-1} \lambda_i^k \xi_i \vv_i
\]
for some coefficients $\Seq{\vv_i}$.

\end{frame}

\begin{frame}
  \frametitle{Power method}

\medskip
Assume that $\lambda_0$ is a dominant real eigenvalue and $\xi_0 \neq 0$, then
\[
\hx^k = \lambda_0^k \bigl(\xi_0 \vv_0 + \rr_k \bigr)
\]
with the residual $\rr_k$ defined as
\[
\rr_k = \lambda_0^{-k} \sum_{i=1}^{N-1} \lambda_i^k \xi_i \vv_i
\]
and $\lim_{k\rightarrow\infty} \rr_k = O_{\xRN}$.
To the limit $\hx_{k+1} \approx \lambda_0 \hx^k \approx \lambda_0\xi_0\vv0$ almost parallel to the first eigenvector.

\end{frame}

\begin{frame}
  \frametitle{Power method}

\begin{itemize}
\item This method is fast to compute the spectral radius for the Jacobi method and relaxation parameters.
\item The convergence is geometric and the speed depends on the ratio $\lvert \lambda_1 / \lambda_0\rvert$.
\item If the matrix is symmetric, the convergence speed can be doubled.
\item If $\lambda_0$ is very large or very small then taking high powers lead to numerical issues, the algorithm requires a normalization.
\end{itemize}

\end{frame}

\begin{frame}
  \frametitle{Software Packages}

Libraries like PETSc and Trilinos offer interfaces to:
\begin{itemize}
\item a wide-range of iterative solvers based on Krylov-spaces,
\item preconditioned by block Jacobi. ILU, AMG, \dots
\item so better design your software packages in consequence.
\end{itemize}
\end{frame}

\input{postamble}
